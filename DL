
Experiment-2 simple neural network for perform classification

import pandas as pd
import numpy as np
from keras.models import Sequential
from keras.layers import Dense

dataset = pd.read_csv('/content/diabetes.csv')
print(dataset.head())
dataset = dataset.apply(pd.to_numeric, errors='coerce')
dataset = dataset.fillna(dataset.mean())
dataset = dataset.values

X = dataset[:, 0:8]
y = dataset[:, 8]

# Define the Keras model
model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the Keras model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Fit the Keras model on the dataset
model.fit(X, y, epochs=150, batch_size=10)

# Evaluate the model
_, accuracy = model.evaluate(X, y)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Make class predictions with the model
predictions = (model.predict(X) > 0.5).astype(int)

# Summarize the first 5 cases
for i in range(5):
    print(f'{X[i].tolist()} => Predicted: {predictions[i][0]}, Actual: {int(y[i])}')






Experiment -6 CNN by tuning hyper parameter

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the CNN model
def create_model(num_filters=32, kernel_size=(3, 3), pool_size=(2, 2), dense_units=64):
    model = models.Sequential([
        layers.Conv2D(num_filters, kernel_size, activation='relu', input_shape=(28, 28, 1)),
        layers.MaxPooling2D(pool_size),
        layers.Conv2D(2*num_filters, kernel_size, activation='relu'),
        layers.MaxPooling2D(pool_size),
        layers.Conv2D(2*num_filters, kernel_size, activation='relu'),
        layers.Flatten(),
        layers.Dense(dense_units, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Create a custom KerasClassifier wrapper
class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, build_fn=None, epochs=5, batch_size=32, **kwargs):
        self.build_fn = build_fn
        self.epochs = epochs
        self.batch_size = batch_size
        self.kwargs = kwargs

    def fit(self, X, y):
        self.model = self.build_fn(**self.kwargs)
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)
        return self

    def predict(self, X):
        return np.argmax(self.model.predict(X), axis=-1)

    def score(self, X, y):
        loss, accuracy = self.model.evaluate(X, y, verbose=0)
        return accuracy

    def set_params(self, **params):
        for key, value in params.items():
            if key == 'build_fn':
                continue
            elif key in ['epochs', 'batch_size']:
                setattr(self, key, value)
            else:
                self.kwargs[key] = value
        return self

# Load and preprocess the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.astype('float32') / 255.0
test_images = test_images.astype('float32') / 255.0
train_images = np.expand_dims(train_images, axis=-1)
test_images = np.expand_dims(test_images, axis=-1)

# Create the KerasClassifierWrapper based on the Keras model
model = KerasClassifierWrapper(build_fn=create_model, epochs=3, batch_size=32)

# Define hyperparameters for randomized search
param_distributions = {
    'num_filters': [32, 64],
    'kernel_size': [(3, 3), (5, 5)],
    'pool_size': [(2, 2), (3, 3)],
    'dense_units': [64, 128]
}

# Create the RandomizedSearchCV object
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions,
                                   n_iter=4, cv=2, verbose=2, n_jobs=1)  # Set n_jobs=1 if facing memory issues

# Perform the randomized search
random_search.fit(train_images, train_labels)

# Get the best parameters and model
best_params = random_search.best_params_
best_model = random_search.best_estimator_

# Evaluate the best model on the test set
test_acc = best_model.score(test_images, test_labels)
print(f"Best parameters: {best_params}")
print(f"Test accuracy of the best model: {test_acc}")






Experiment -5 CNN for image classification

# exp 5
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
import numpy as np

# Load the MNIST dataset and preprocess it
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the pixel values to range [0, 1]
train_images = train_images.astype('float32') / 255.0
test_images = test_images.astype('float32') / 255.0

# Add a channel dimension for grayscale images (required for CNN)
train_images = np.expand_dims(train_images, axis=-1)
test_images = np.expand_dims(test_images, axis=-1)

# Define the CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Print the model summary
model.summary()

# Train the model
history = model.fit(train_images, train_labels, epochs=5, batch_size=64,
                    validation_split=0.1)

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test accuracy: {test_acc}")







Experiment -7 Auto encoder for dimensionality reduction

# exp7
import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras import layers, models
import numpy as np
import matplotlib.pyplot as plt

# Load the Fashion MNIST dataset and preprocess it
(train_images, _), (test_images, _) = fashion_mnist.load_data()

# Normalize the pixel values to range [0, 1]
train_images = train_images.astype('float32') / 255.0
test_images = test_images.astype('float32') / 255.0

# Flatten the images for the autoencoder
train_images_flat = train_images.reshape((len(train_images), np.prod(train_images.shape[1:])))
test_images_flat = test_images.reshape((len(test_images), np.prod(test_images.shape[1:])))

# Define the autoencoder model
def build_autoencoder(encoding_dim):
    input_img = layers.Input(shape=(784,))

    # Encoder
    encoded = layers.Dense(encoding_dim, activation='relu')(input_img)

    # Decoder
    decoded = layers.Dense(784, activation='sigmoid')(encoded)

    # Autoencoder
    autoencoder = models.Model(input_img, decoded)

    # Separate encoder model
    encoder = models.Model(input_img, encoded)

    # Separate decoder model
    encoded_input = layers.Input(shape=(encoding_dim,))
    decoder_layer = autoencoder.layers[-1]
    decoder = models.Model(encoded_input, decoder_layer(encoded_input))

    return autoencoder, encoder, decoder

# Define the size of the encoding dimension (reduce from 784 to 32 dimensions)
encoding_dim = 32

# Build the autoencoder model
autoencoder, encoder, decoder = build_autoencoder(encoding_dim)

# Compile the autoencoder
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# Train the autoencoder
history = autoencoder.fit(train_images_flat, train_images_flat,
                          epochs=10,
                          batch_size=256,
                          shuffle=True,
                          validation_data=(test_images_flat, test_images_flat))

# Encode and decode the test images
encoded_imgs = encoder.predict(test_images_flat)
decoded_imgs = decoder.predict(encoded_imgs)

# Display some test images and their reconstructed counterparts
n = 3  # number of images to display
plt.figure(figsize=(20, 4))
for i in range(n):
    # Original images
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(test_images[i])
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # Reconstructed images
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()







Experiment -9 Sentiment analysis using LSTM

# Importing necessary libraries
import numpy as np
import pandas as pd
import re
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Example dataset with texts and emojis
data = {
    'text': [
        'I love this! â¤ï¸',
        'Baseball is great! âš¾',
        'I am so happy! ðŸ˜',
        'This is scary! ðŸ”ª'
    ],
    'label': [
        'positive',
        'positive',
        'positive',
        'negative'
    ]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Preprocessing function to clean the text
def clean_text(text):
    # Remove unnecessary characters except emojis
    text = re.sub(r'[^a-zA-Z0-9\sâ¤ï¸âš¾ðŸ˜ðŸ”ª]', '', text)
    return text

df['clean_text'] = df['text'].apply(clean_text)

# Tokenizing text (including emoji)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['clean_text'])
vocab_size = len(tokenizer.word_index) + 1  # +1 for padding

# Convert text to sequences
sequences = tokenizer.texts_to_sequences(df['clean_text'])
maxlen = 10  # Max length of input sequence
X = pad_sequences(sequences, maxlen=maxlen)

# Encode labels (positive/negative)
encoder = LabelEncoder()
y = encoder.fit_transform(df['label'])

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the LSTM model
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=maxlen))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test accuracy: {accuracy*100:.2f}%')

# Predict on new text
new_texts = ['I am so excited! ðŸ˜', 'This is terrible! ðŸ”ª']
new_sequences = tokenizer.texts_to_sequences(new_texts)
new_padded = pad_sequences(new_sequences, maxlen=maxlen)

predictions = model.predict(new_padded)
for text, prediction in zip(new_texts, predictions):
    sentiment = 'positive' if prediction > 0.5 else 'negative'
    print(f'Text: {text} | Sentiment: {sentiment}')







Experiment - 10 LSTM by hyper parameter

# Importing necessary libraries
import numpy as np
import pandas as pd
import re
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Example dataset with texts and emojis
data = {
    'text': [
        'I love this! â¤ï¸',
        'Baseball is great! âš¾',
        'I am so happy! ðŸ˜',
        'This is scary! ðŸ”ª'
    ],
    'label': [
        'positive',
        'positive',
        'positive',
        'negative'
    ]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Preprocessing function to clean the text
def clean_text(text):
    # Remove unnecessary characters except emojis
    text = re.sub(r'[^a-zA-Z0-9\sâ¤ï¸âš¾ðŸ˜ðŸ”ª]', '', text)
    return text

df['clean_text'] = df['text'].apply(clean_text)

# Tokenizing text (including emoji)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['clean_text'])
vocab_size = len(tokenizer.word_index) + 1  # +1 for padding

# Convert text to sequences
sequences = tokenizer.texts_to_sequences(df['clean_text'])
maxlen = 10  # Max length of input sequence
X = pad_sequences(sequences, maxlen=maxlen)

# Encode labels (positive/negative)
encoder = LabelEncoder()
y = encoder.fit_transform(df['label'])

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the LSTM model with tuned hyperparameters
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=maxlen))

# Increasing LSTM units and adding Dropout
model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3))  # Increased units and dropout rate
model.add(Dropout(0.3))  # Additional dropout for regularization
model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification

# Compile the model with an optimized learning rate and different optimizer
optimizer = Adam(learning_rate=0.001)  # Adjust learning rate if needed
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model with tuned batch size and increased epochs
model.fit(X_train, y_train, batch_size=16, epochs=10, validation_data=(X_test, y_test))  # Reduced batch size, increased epochs

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test accuracy: {accuracy*100:.2f}%')

# Predict on new text
new_texts = ['I am so excited! ðŸ˜', 'This is terrible! ðŸ”ª']
new_sequences = tokenizer.texts_to_sequences(new_texts)
new_padded = pad_sequences(new_sequences, maxlen=maxlen)

predictions = model.predict(new_padded)
for text, prediction in zip(new_texts, predictions):
    sentiment = 'positive' if prediction > 0.5 else 'negative'
    print(f'Text: {text} | Sentiment: {sentiment}')








Experiment -4 neural network with hyper parameter tuning

# Import necessary libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from scikeras.wrappers import KerasClassifier

# Load the dataset using pandas to handle the header row
data = pd.read_csv('/content/diabetes.csv')

# Split into input (X) and output (y) variables
X = data.iloc[:, 0:8].values  # Input features
y = data.iloc[:, 8].values    # Output label

# Define the keras model
def create_model2(activation='relu'):
    model = Sequential()
    model.add(Dense(12, input_shape=(8,), activation=activation))  # Use the passed activation
    model.add(Dense(8, activation=activation))
    model.add(Dense(1, activation='sigmoid'))
    # Compile the keras model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Create model
model = KerasClassifier(model=create_model2, epochs=150, batch_size=10, verbose=0)

# Define the grid search parameters
activation = ['softmax', 'relu', 'tanh', 'sigmoid', 'linear']
param_grid = dict(model__activation=activation)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)

# Fit the model using grid search
grid_result = grid.fit(X, y)

# Summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))








Experiment - 3 multi layer neural network with diff activation fn

# Import necessary libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import KFold
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from scikeras.wrappers import KerasClassifier  # Use scikeras for KerasClassifier
from sklearn.metrics import accuracy_score

# Load the dataset using pandas to handle the header row
data = pd.read_csv('/content/diabetes.csv')

# Split into input (X) and output (y) variables
X = data.iloc[:, 0:8].values  # Input features
y = data.iloc[:, 8].values    # Output label

# Define the keras model
def create_model(activation='relu'):
    model = Sequential()
    model.add(Dense(12, input_shape=(8,), activation=activation))  # Use the passed activation
    model.add(Dense(8, activation=activation))
    model.add(Dense(1, activation='sigmoid'))
    # Compile the keras model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Create KerasClassifier
model = KerasClassifier(model=create_model, epochs=150, batch_size=10, verbose=0)

# K-Fold Cross Validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
accuracies = []

# Loop through each fold
for train_index, test_index in kfold.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    # Fit the model
    model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred = (model.predict(X_test) > 0.5).astype(int)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

# Summarize results
mean_accuracy = np.mean(accuracies)
std_accuracy = np.std(accuracies)

print(f"Mean Accuracy: {mean_accuracy:.4f}")
print(f"Standard Deviation: {std_accuracy:.4f}")



Experiment-8 Object Detection

import tensorflow as tf
from tensorflow.keras import layers, models
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

# CNN Model for Feature Extraction
def create_cnn(input_shape):
    inputs = layers.Input(shape=input_shape)
    x = layers.Conv2D(32, (3, 3), activation='relu')(inputs)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Conv2D(64, (3, 3), activation='relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Conv2D(128, (3, 3), activation='relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Flatten()(x)

    return inputs, x

# Model for Object Detection
def create_object_detection_model(input_shape, num_classes):
    inputs, cnn_output = create_cnn(input_shape)

    # Classification Head (Softmax)
    class_output = layers.Dense(num_classes, activation='softmax', name='class_output')(cnn_output)

    # Bounding Box Regression Head (4 outputs: x, y, width, height)
    bbox_output = layers.Dense(4, name='bbox_output')(cnn_output)

    # Create the final model with two outputs (classification and bounding box regression)
    model = models.Model(inputs=inputs, outputs=[class_output, bbox_output])

    return model

# Function to preprocess the data (normalize images, select the first object's bounding box and label)
def preprocess(data):
    image = tf.image.resize(data['image'], (128, 128)) / 255.0  # Resize and normalize the image

    # Only consider the first object in the image for simplicity
    bbox = data['objects']['bbox'][0]  # Use the first bounding box
    label = data['objects']['label'][0]  # Use the first object's label

    bbox = tf.cast(bbox, tf.float32)  # Bounding box as float32
    label = tf.one_hot(label, depth=num_classes)  # One-hot encode the label for classification

    return image, (label, bbox)  # Return label and bbox separately

# Function to display images with bounding boxes and labels
def display_samples(dataset, num_samples, class_names):
    plt.figure(figsize=(15, 15))
    for i, (image, (label, bbox)) in enumerate(dataset.take(num_samples)):
        plt.subplot(1, num_samples, i + 1)
        plt.imshow(image.numpy())

        # Scale bbox back to image size (since the image is resized to 128x128)
        height, width, _ = image.shape
        bbox_scaled = [bbox[0] * height, bbox[1] * width, bbox[2] * height, bbox[3] * width]

        # Draw a bounding box (x, y, width, height)
        rect = plt.Rectangle((bbox_scaled[1], bbox_scaled[0]),  # Top-left corner (y, x)
                             bbox_scaled[3] - bbox_scaled[1],  # Width
                             bbox_scaled[2] - bbox_scaled[0],  # Height
                             fill=False, color='red', linewidth=2)
        plt.gca().add_patch(rect)

        # Find the class index and label
        class_index = tf.argmax(label).numpy()
        class_label = class_names[class_index]

        # Display the object label
        plt.text(bbox_scaled[1], bbox_scaled[0] - 10, class_label, color='blue', fontsize=12, backgroundcolor='white')

        plt.axis('off')
    plt.show()

# Parameters
input_shape = (128, 128, 3)  # Image dimensions
num_classes = 21  # 20 object classes + 1 background

# Load the PASCAL VOC 2007 dataset
dataset, info = tfds.load("voc/2007", split='train', with_info=True)
class_names = info.features['objects']['label'].names  # Get the class names

train_dataset = dataset.map(preprocess).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

# Display some samples from the dataset with object labels and bounding boxes
display_samples(dataset.map(preprocess), num_samples=5, class_names=class_names)

# Create the object detection model
model = create_object_detection_model(input_shape, num_classes)

# Compile the model with separate losses for classification and bounding box regression
model.compile(optimizer='adam',
              loss={'class_output': 'categorical_crossentropy', 'bbox_output': 'mean_squared_error'},
              metrics={'class_output': 'accuracy'})

# Display the model architecture
model.summary()

# Train the model
history = model.fit(train_dataset, epochs=10)

# Plot training & validation accuracy and loss
def plot_history(history):
    print(history.history.keys())  # Check available keys

    # Accuracy
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['class_output_accuracy'], label='Classification Accuracy')
    plt.title('Classification Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Losses
    plt.subplot(1, 2, 2)


    plt.plot(history.history['loss'], label='Total Loss')

    # Adjust if loss names are different
    if 'class_output_loss' in history.history:
        plt.plot(history.history['class_output_loss'], label='Classification Loss')
    if 'bbox_output_loss' in history.history:
        plt.plot(history.history['bbox_output_loss'], label='Bounding Box Loss')

    plt.title('Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Call the function to plot the graph
plot_history(history)


